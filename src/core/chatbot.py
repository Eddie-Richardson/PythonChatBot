# src/core/chatbot.py
"""
Core chat orchestration logic for the AI Academic & Coding Assistant.

Responsibilities:
- Build the message payload for the LLM, including:
    - System prompt
    - Relevant knowledge base snippets
    - Recent conversation history
    - Current user input
- Query the LLM (Cerebras) for a reply
- Save conversation turns to persistent history
- Optionally summarize long histories

Local-only notes:
- Currently hardwired to CerebrasClient; LLM_PROVIDER is imported but unused.
- No error handling for LLM failures or empty KB results.
- Summarization uses HFClient(), which is undefined in this file — likely a leftover from pre‑Cerebras code.
"""

from typing import List, Dict
from src.config import LLM_PROVIDER, MAX_TOKENS
from src.core.prompts import SYSTEM_ACADEMIC_CODING
from src.llm.cerebras_client import CerebrasClient
from src.memory.convo_store import save_message, get_history
from src.memory.kb_store import search
from src.memory.convo_store import get_history, save_message, clear_history
from src.memory.archive_store import archive_messages

def _client():
    """
    Factory for the active LLM client.
    TODO: Make this dynamic based on LLM_PROVIDER instead of hardcoding CerebrasClient.
    """
    return CerebrasClient()

def build_messages(workspace: str, user_id: str, user_input: str, kb_snippets: List[str]):
    """
    Construct the list of messages to send to the LLM.

    Args:
        workspace: Logical grouping for KB + history (e.g., course/repo name)
        user_id:   Unique user identifier (Discord ID, etc.)
        user_input: The latest message from the user
        kb_snippets: List of relevant text chunks from the KB

    Returns:
        List of message dicts in OpenAI/Cerebras chat format.
    """
    # Start with the system prompt
    messages: List[Dict[str, str]] = [{"role": "system", "content": SYSTEM_ACADEMIC_CODING}]

    # Add KB context if available
    if kb_snippets:
        context = "\n\n".join(f"- {s}" for s in kb_snippets)
        messages.append({"role": "system", "content": f"Context from knowledge base:\n{context}"})

    # Append recent conversation history (up to 20 turns)
    messages += get_history(workspace, user_id, limit=20)

    # Append the current user message
    messages.append({"role": "user", "content": user_input})

    return messages

def retrieve_kb(workspace: str, query: str, k: int = 5):
    """
    Search the workspace's KB for relevant chunks.

    Args:
        workspace: KB namespace
        query:     Search query (usually the user's message)
        k:         Max number of results to return

    Returns:
        List of text strings from the KB.
    """
    hits = search(workspace, query, k=k)
    texts = [h["text"] for h in hits]
    return texts

def chat_once(workspace: str, user_id: str, user_input: str) -> str:
    """
    Handle a single-turn chat interaction:
    - Retrieve KB context
    - Build full message list
    - Query LLM for a reply
    - Save both user and assistant messages to active history
    - Auto-summarize history if it’s getting long (hybrid mode)
    """
    # Step 1: Retrieve relevant KB snippets
    kb = retrieve_kb(workspace, user_input, k=5)

    # Step 2: Build the message payload
    messages = build_messages(workspace, user_id, user_input, kb)

    # Step 3: Query the LLM
    reply = _client().chat(messages, max_tokens=MAX_TOKENS)

    # Step 4: Save conversation turns to active store
    save_message(workspace, user_id, "user", user_input)
    save_message(workspace, user_id, "assistant", reply)

    # Step 5: Check if summarization is needed
    summarize_history_if_long(workspace, user_id)

    return reply


def summarize_history_if_long(workspace: str, user_id: str):
    """
    Hybrid summarization:
    - If active history >= 40 messages, archive older turns and replace them with a summary.
    - Keeps the last 10 turns verbatim in the active store.
    - Summary is generated by the active LLM client.
    """
    hist = get_history(workspace, user_id, limit=60)
    if len(hist) < 40:
        return

    # Split into old and recent messages
    old_msgs = hist[:-10]
    recent_msgs = hist[-10:]

    # Archive the old messages for long-term storage
    archive_messages(workspace, user_id, old_msgs)

    # Build summarization prompt
    prompt = [
        {
            "role": "system",
            "content": (
                "Summarize the prior conversation into concise bullet points "
                "capturing facts, decisions, and TODOs."
            )
        },
        {
            "role": "user",
            "content": "\n\n".join(f"{m['role']}: {m['content']}" for m in old_msgs)
        }
    ]

    # Generate summary with the active LLM client
    summary = _client().chat(prompt, max_tokens=220)

    # Clear active history and replace with summary + recent turns
    clear_history(workspace, user_id)
    save_message(workspace, user_id, "system", f"Conversation summary:\n{summary}")
    for m in recent_msgs:
        save_message(workspace, user_id, m["role"], m["content"])
